\chapter{Multi-Parameter Models}
In the previous chapter, we introduced the idea of Bayesian Statistics, two one-parameter models, and one-parameter exponential family. In this chapter, we will go explore models with multiple parameters, together with some advanced ideas. 

\section{The most objective: Jeffreys' Prior}
When we are trying to select a prior, we often turn to the uniform distribution if we have no idea about how the parameters are distributed. Uniform is believed to be objective because all possible parameters are assigned with the same weight, meaning we are not favoring any particular options. But it's far from being perfect, because if so, this section won't exist. In this section, we will first explain why uniform can be problematic sometimes, and then introduce the Jeffreys' prior, which is believed to be more objective to some.

One of the good sides of using uniform as prior is that, as long as you are selecting the parameter(s) within a finite region, you don't need to adjust the weight since it will be offset in the normalization factor afterall. However, if $|\Omega| = \infty$ for example $[0, +\infty), (-\infty, 0]$, then the integration of the prior is bound to be infinite. The problem is that, not only the decomposition $\mathbf{P}(Y=y) = \int_{\Omega} \mathbf{P}(Y=y|\theta) d\theta$ will very likely to be invalid because the integration might not converge. In addition, even if it converges, uniform prior with $\pi(\theta) = 1$ becomes less objective because now it tends to favor larger $\theta$ since for all finite interval/region $I$, $\int_{I} \pi(\theta) d\theta < \infty$ but $\int_{\Omega\setminus I}\pi(\theta)d\theta = \infty$. 

Apart from infinite regions, uniform prior also doesn't work well with change of variables. Take the following as an example:
\begin{example}
    Let $\theta \sim Unif(0,1)$ and odds $\tau = \frac{\theta}{1-\theta}$. Then we have $\theta = \frac{\tau}{1+\tau}$ and $\frac{d\theta}{d\tau} = \frac{1}{(1+\tau)^2}$. Using the formula for  change of variable, we have $pdf(\tau) = \pi(\theta(\tau))\frac{d\theta}{d\tau} = \frac{1}{(1+\tau)^2},\; \tau \in (0, +\infty)$.
\end{example}

To address these issues, mostly on the second one, we introduce Jeffreys' Prior
\begin{equation*}
    \pi(\theta) = \sqrt{I(\theta)}
\end{equation*}
where $I(\theta)$ is the \emph{Fisher Information} with $I(\theta) = - \mathbb{E}[\frac{\partial^2 \log p(y|\theta)}{\partial\theta^2} | \theta] = -\mathbb{E}[\frac{\partial^2 \log L}{\partial \theta^2} | \theta]$. It's not intuitive at the first glance, but the following property enbales Fisher Information to be a part of the prior.

\begin{proposition}
    \begin{equation*}
        I(\theta) = - \mathbb{E}[\frac{\partial^2 \log L}{\partial \theta^2}] = \mathbb{E}[( \frac{\partial \log L}{\partial \theta} )^2]
    \end{equation*}
\end{proposition}

With this property, we can prove that Jeffreys' prior is invariable under change of variables. In other words, we have:
\begin{theorem}
    With $\phi = \phi(\theta)$ and $p(\theta) \propto \sqrt{I(\theta)}$, we have $p(\phi) \propto \sqrt{I(\phi)}$.
\end{theorem}

One question one may have right now is why is Jeffreys' prior objective? Clearly it doesn't assign the same weight on all possible parameter candidates since it's not a uniform. The answer is, Jeffreys' Prior is in fact the maximizer of the KL-divergence bewteen the prior and posterior distribution. In this sense, Jeffreys' prior is objective because it "allows" the data to speak the most of it. Apparently, this idea can be philosophical and up to individual's personal perspective, but at least mathematically it's a handy wrench in the toolbox by introducing invariability to reparametrization. 

\begin{note}[The maximizer of KL-divergence]
    TBD
\end{note}

\section{The Normal Model}
Normal distribution will be the first multi-parameter model we are going to study. The methodology is intuitive, instead of modeling two parameters at a time, we first fix $\sigma^2$ as a constant, which now equals to a one-parameter model. Then, we introduce $\sigma^2$ as a random variable and repeat the procedure. 

\subsection{Condition Posterior}
We have joint sampling density as the product of $n$ normal due to conditional independence
\begin{align}
    \mathbf{P}(y_1, \ldots, y_n | \theta, \sigma^2) &\propto \prod_{i=1}^{n}\exp(-\frac{1}{2\sigma^2}(y_i - \theta)^2) \\
    &=\exp(-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i - \theta)^2) \\
    &=\exp(-\frac{1}{2}(\frac{n}{\sigma^2}\theta^2 - 2\frac{\sum y_i}{\sigma^2} + \frac{\sum y_i^2}{\sigma^2})) \label{eq:NormalConditionalSamplingDecomposition}\\
    &=\exp(c_1(\theta - c_2)^2 + c_3)
\end{align}

By fixing $\sigma^2$ as a constant, we have conditional posterior
\begin{align}
    \mathbf{P}(\theta | y_1, \ldots, y_n, \sigma^2) &\propto \mathbf{P}(y_1, \ldots, y_n | \theta, \sigma^2) \cdot \mathbf{P}(\theta | \sigma^2)  \label{eq:NormalConditionalPosterior}
\end{align}

To make it conjugate, one straightforward choice for prior is also normal, specifically $N(\mu_0, \tau_0^2)$. Then we have the following decomposition of the prior pdf:
\begin{align}
    \mathbf{P}(\theta | \sigma^2) &\propto \exp(- \frac{1}{2\tau_0^2}(\theta - \mu_0)) \\
    &=\exp(-\frac{1}{2}(\frac{1}{\tau_0^2}\theta^2 - 2\frac{\mu_0}{\tau_0^2}\theta + \frac{\mu_0^2}{\tau_0^2})) \label{eq:NormalConditionalPriorDecomposition}\\
    &=\exp(-\frac{1}{2}(a\theta^2 - 2b\theta^2 +c))
\end{align}
This expression offers us a quick way to compute the mean and gradient of the normal distribution:
\begin{equation}
    \tau_0^2 = \frac{1}{a}, \quad \mu_0 = b \cdot \tau_0^2 = \frac{b}{a} \label{eq:QuickNormalMeanVarFormula}
\end{equation}

Now we have collected all of the ingredients we need, by combining (\ref{eq:NormalConditionalSamplingDecomposition}) and (\ref{eq:NormalConditionalPriorDecomposition}) into (\ref{eq:NormalConditionalPosterior}), we obtain the following decomposition:
\begin{equation}
    \mathbf{P}(\theta | y_1, \ldots, y_n, \sigma^2) \propto \exp(-\frac{1}{2}\big[ (\frac{n}{\sigma^2} + \frac{1}{\tau_0^2})\theta^2 - 2(\frac{\sum y_i}{\sigma^2} + \frac{\mu_0}{\tau_0^2})\theta + (\frac{\sum y_i^2}{\sigma^2} + \frac{\mu_0^2}{\tau_0^2}) \big])
\end{equation}

By using the formula (\ref{eq:QuickNormalMeanVarFormula}) and substituting the variance terms with precision, we obtain conditional posterior mean and variance:
\begin{equation*}
    \tau_n^2 = \frac{1}{a} = \frac{1}{n\tilde{\sigma}^2 + \tilde{\tau_0}^2}, \quad \mu_n = \frac{b}{a} = \frac{\sum y_i \tilde{\sigma}^2 + \mu_0\tilde{\tau_0}^2}{n\tilde{\sigma}^2 + \tilde{\tau_0}^2}
\end{equation*}
which means we have the conditional postertior
\begin{equation*}
    \theta | y_1, \ldots, y_n, \sigma^2 \sim N(\mu_n, \tau_n^2)
\end{equation*}

\subsection{Law of Total Expectation/Variance}
Conditional expectation $\mathbb{E}[U|V=v]$ is a random variable in $V$, which means its value changes according to $V$. The law of total expectation states that 
\begin{equation*}
    \mathbb{E}[\mathbb{E}[U|V]] = \mathbb{E}[U]
\end{equation*}\
The proof is quite simple, by writing the left hand side explicitly:
\begin{align*}
    &\int_{V}\int_{U|V=v}u \cdot p(U=u|V=v) du \cdot p(V=v)dv \\
    =& \int_{V}\int_{U|V=v}u \cdot p(U=u) dudv \\
    =& \int_{V}\int_{U}u \cdot p(U=u) dudv \\
    =& \int_{V} \mathbb{E}[U] dv = \mathbb{E}[U]
\end{align*}

By doing something similar, we derive the law of total variance:
\begin{equation*}
    Var(U) = \mathbb{E}[Var(U|V)] + Var(\mathbb{E}[U|V])
\end{equation*}

\subsection{Predictive Conditional Posterior Distribution}
In this part we will model the predictive model. Conditioned on $\sigma^2$, we have
\begin{align*}
    \mathbf{P}(\tilde{Y} | y_1, \ldots, y_n, \sigma^2) &= \int \mathbf{P}(\tilde{Y},\theta | y_1, \ldots, y_n, \sigma^2) \cdot \mathbf{P}(\theta | y_1, \ldots, y_n, \sigma^2) d\theta \\
    &= \int \underbrace{\mathbf{P}(\tilde{Y},\theta | \sigma^2)}_{\text{sampling model: normal}} \cdot \underbrace{\mathbf{P}(\theta | y_1, \ldots, y_n, \sigma^2)}_{\text{conditional prior: normal}} d\theta
\end{align*}

This tells us that, the conditional predictive distribution is also normal with some mean and variance. In the following we are going to find out what they are using the laws we mentioned above. 

\begin{align*}
    \text{mean} &= \mathbb{E}[\tilde{Y} | \vec{Y}, \sigma^2] \\
    &=\mathbb{E}[\mathbb{E}[\tilde{Y} | \theta, \vec{Y}, \sigma^2] | \vec{Y}, \sigma^2] \\
    &=\mathbb{E}[\theta | \vec{Y}, \sigma^2] \\
    &=\mu_n
\end{align*}

\begin{align*}
    \text{Var} &= Var(\tilde{Y} | \vec{Y}, \sigma^2) \\
    &= \mathbb{E}[ Var(\tilde{Y} | \vec{Y}, \theta, \sigma^2) | \vec{Y}, \sigma^2] + Var(\mathbb{E}[\tilde{Y} | \vec{Y}, \theta, \sigma^2] | \vec{Y}, \sigma^2) \\
    &= \mathbb{E}[\sigma^2 | \vec{Y}, \sigma^2] + Var(\theta | \vec{Y}, \sigma^2) \\
    &= \sigma^2 + \tau_n^2
\end{align*}

This implied that $\tilde{Y} | y_1, \ldots, y_n, \sigma^2 \sim N(\mu_n, \sigma^2 + \tau_n^2)$.

\subsection{Joint Inference for Mean and Variance}
Previously, we derived our work by fixing $\sigma^2$. In this section, we will find out what's going to happen if we bring $\sigma^2$ back to life. Firstly, it is natural to write the joint posterior as

\begin{equation*}
    \mathbf{P}(\theta, \sigma^2 | y_1, \ldots, y_n) \propto \mathbf{P}(y_1, \ldots, y_n | \theta, \sigma^2) \cdot \mathbf{P}(\theta | \sigma^2) \cdot \mathbf{P}(\sigma^2)
\end{equation*}
where the first two terms are already known to be $N(\theta, \sigma^2)$ and $N(\mu_0, \tau_0^2)$. It is obvious that if $\sigma^2$ also follows normal, then the posterior will also be a normal. However, this is impossible because $\sigma^2$ must be non-negative even in the loosest case, where normal distribution is not legitimate.  

To address this issue, we decomopse the posterior in a different way and assume a specific dependence of $\theta$ on $\sigma^2$.

\begin{align*}
    \frac{1}{\sigma^2} &\sim Gamma(\frac{\nu_0}{2}, \frac{\nu_0\sigma_0^2}{2}) \\
    \sigma^2 &\sim InvGamma(\frac{\nu_0}{2}, \frac{\nu_0\sigma_0^2}{2}) \\
\end{align*}

We have decomposition:
\begin{align*}
    \mathbf{P}(\theta, \sigma^2 | y_1, \ldots, y_n) &\propto \mathbf{P}(\theta | \vec{Y}, \sigma^2) \cdot \mathbf{P}(\sigma^2 | \vec{Y}) \\
    &= N(\mu_n, \tau_n^2) \cdot \mathbf{P}(\sigma^2 | \vec{Y})
\end{align*}
where we can further write the second term in the following way:
\begin{align*}
    \mathbf{P}(\sigma^2 | \vec{Y}) &\propto \mathbf{P}(\vec{Y}, \sigma^2) \cdot \mathbf{P}(\sigma^2) \\
    &= \mathbf{P}(\sigma^2) \cdot \int \mathbf{P}(\vec{Y} \theta | \sigma^2) d\theta \\
    &= \mathbf{P}(\sigma^2) \cdot \int \mathbf{P}(\vec{Y} | \theta, \sigma^2) \cdot \mathbf{P}(\theta | \sigma^2) d\theta \\
    &=InvGamma \int Normal \cdot Normal \;d\theta \\
    &= InvGamma(\frac{\nu_n}{2}, \frac{\nu_n\sigma_n}{2})
\end{align*}
where $\nu_n = \nu_0 + n$, $\nu_n\sigma_n^2 = \nu_0\sigma_0^2 + (n-1)s^2 + \frac{\kappa_0n}{\kappa_0 + n}(\bar{y} - y_0)^2$, $\kappa_0 = \frac{\sigma^2}{\tau_0^2}$. Here $\kappa_0$ can be interpreted as the prior sample size. 

\subsection*{Semi-Conjugate Case}
In the previous example, by modeling the normal model to be \emph{normal-inverse-gamma}, we obtained a full-conjugate configuration. We have $\theta | \sigma^2, \vec{Y}$ samely distributed as $\theta | \sigma^2$ as normal, and $\sigma^2 | \vec{Y}$ samely distributed as $\sigma^2$ as inverse gamma. What's more important is that, this kind of structure allows the parameters to be implicitly connected, i.e. having $\theta$ dependent on $\sigma^2$.

However, this might not always be the case in real life. In some cases, a priori we may believe that $\theta$ and $\sigma^2$ are in fact independent to each other, i.e. $\theta = \theta | \sigma^2$. We refer to this situation or this kind of modeling structure as semi-conjugate. While it still allows conjugate behaviors, the core difference compared with full-conjugate is the way we model the distribution of parameters. 

\subsection{Clarification on Some Terms}
Many different notations popped in our previous discussion over normal models which can be confusing for first time readers. This section specifically addresses this issue. We will discuss and categorize the notations mentioned above for clarification purposes. 

It's almost a mess when we are looking at $\theta, \sigma^2, \mu_0, \mu_n, \tau_0^2, \tau_n^2, \nu_0, \nu_n$ altogether, even with the context. As a matter of fact, they can be taken and categorized in the following ways:

Firstly, $\theta, \sigma^2$ are the parameters of our true interest. It is the posterior of these two parameters that we truly care about. These two determines the sampling/predictive model. 
\begin{equation*}
    \tilde{Y} | \vec{Y} = \tilde{Y} | \theta, \sigma^2 \sim N(\theta, \sigma^2)
\end{equation*}

Secondly, the $(\mu_n, \tau_n)$ pair. This notation was introduced when we were studying the behavior when conditioned on fixed $\sigma^2$. In other words, they are the parameters of the conditional posterior of $\theta$
\begin{equation*}
    \theta | \vec{Y}, \sigma^2 \sim N(\mu_n, \tau_n^2)
\end{equation*}
And as for the initial pair $(\mu_0, \tau_0)$, they were the parameters for the prior
\begin{equation*}
    \theta | \sigma^2 \sim N(\mu_0, \tau_0^2)
\end{equation*}

Similarly, $\nu_0$ and $\nu_n$ are the parameters for the prior and posterior of $\sigma^2$ after we reintroduce $\sigma^2$ as a random variable. 

From my current understanding and the lecture, $\kappa_0$ is understood as the prior sample size, which can be confusing since by definition this can be a decimal, and act as a relation between the params of two distribution without any necassary implicit relations.

\subsection*{So Where is the dependency in prior?}
This is a natural question to ask if we go back to how we defined the conditional prior $\theta | \sigma^2$. Because in the distribution
\begin{equation*}
    \theta | \sigma^2 \sim \mathcal{N}(\mu_0, \tau_0^2)
\end{equation*}
both $\mu_0, \tau_0^2$ came from expert opinion which \emph{should} be independent from the parameter $\sigma^2$. 

However, later on when we were computing the posterior, we defined $\kappa_0 = \frac{\sigma^2}{\tau_0^2}$ and reparametrized the conditional prior to be
\begin{equation*}
    \theta | \sigma^2 \sim \mathcal{N}(\mu_0, \frac{\sigma^2}{\kappa_0})
\end{equation*}
In other words, instead of thinking the experts are giving direct conjecture over the prior's variance, we can interpret it as giving relative relation to the latent $\sigma^2$, and this is where the dependency kicks in. 

\subsection{Monte Carlo Simulation and Marginal Posterior}
To simulate the joint posterior distribution, since we assumed $\theta$'s dependency on $\sigma^2$, we first sample $\sigma^2$ from an InvGamma distribution, and then sample $\theta | \sigma^2$ from $\mathcal{N}(\mu_n, \tau_n^2)$ which depends on $\sigma^2$. This means when we are sampling the parameter pair $(\theta, \sigma^2)$, it follows
\begin{equation*}
    \mathbf{P}(\theta, \sigma^2) = \mathbf{P}(\sigma^2) \cdot \mathbf{P}(\theta | \sigma^2)
\end{equation*}

This looks nice, but what if we don't care about $\sigma^2$ and only want to sample a sequence of $\{\theta_i\}$? Then the above simulation can be expensive since we are running an additional sampling for $n$ times. A faster approach is to directly sample $\theta$ from its marginal posterior distribution
\begin{equation*}
    t(\theta) = \frac{\theta - \mu_n}{\sigma_n/\sqrt{\kappa_n}} | \vec{Y} \sim t_{\nu_0 + n}
\end{equation*}
where $\mu_n = \frac{\kappa_0 \mu_0 + n\bra{y}}{\kappa_0 + n}$ and $\sigma_n^2 = \frac{1}{\nu_0 + n}\big[ \nu_0\sigma_0^2 + (n-1)s^2 + \frac{\kappa_0 n}{\kappa_0 + n}(\bar{y} - \mu)^2 \big]$. In other words, we have the marginal posterior of $\theta$ as a t-distribution. 

\subsection{Weak but Improper Priors}
To find out how a weak prior affects the posterior, we set both $\kappa_0, \nu_0 \rightarrow 0$. But this results in improper priors:
\begin{align*}
    \mathbf{P}(\theta, \sigma^2) &= \mathbf{P}(\theta | \sigma^2) \cdot \mathbf{P}(\sigma^2) \\
    &= \mathcal{N}(\mu_0, \frac{\sigma^2}{\kappa_0}) \cdot IG(\frac{\nu_0}{2}, \frac{\nu_0\sigma_0^2}{2}) \\
    &\rightarrow \mathcal{N}(\mu_0, \infty) \cdot IG(0, 0)
\end{align*}

However, the posterior is still proper since we have
\begin{equation*}
    \mu_n \rightarrow \bar{y} \qquad \sigma_n^2 \rightarrow \frac{1}{n}\sum(Y_i - \bar{Y})^2 \qquad \nu_n \rightarrow n
\end{equation*}

\subsection*{Let's Get Practical}
To solve a real-world problem, we are usually given a belif on what should the parameters be like. One way to encode this information is to ensure that for the joint prior $p(\theta, \sigma^2)$ satisfies the following:
\begin{equation*}
    \mathbb{E}[\sigma^2] = \sigma_0^2 \qquad \mathbb{E}[\theta] = \mu_0
\end{equation*}
One possible parametrization is :
\begin{equation*}
    \theta | \sigma^2 \sim \mathcal{N}(\mu_0, \frac{\sigma^2}{n}) \qquad \sigma^2 \sim InvGamma(\frac{n_0 + 3}{2}, \frac{(n_0 + 1)\sigma_0^2}{2})
\end{equation*}
It can be verified that this joint distribution indeed satisfies the above requirements.