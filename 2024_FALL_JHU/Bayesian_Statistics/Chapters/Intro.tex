\chapter{Introduction and Notation}

The core difference between a Bayesian and a frenquencist is the belief on whether the latent or the parameter $\theta$ is a random variable or a constant. One of the major impacts is that iid assumptions changes to conditional independence instead of mutual independence due to the connection between $\theta$ and $Y$ through the joint distribution $\bf{P}(Y_i, \theta_j)$, where the marginal distribution of $Y_i$ are parametrized by $\theta$. Before everything, we should introduce some simple notations.

\section{Notation}
\begin{enumerate}
    \item[-] $\mathcal{Y}$ denote the set of all possible observation values
    \item[-] $Y$ denote the random variable
    \item[-] $y$ denote the value of a single observation
    \item[-] $\Theta$ is the space of parameters
\end{enumerate}

\begin{note}
    Let $\theta \in \Theta$, define $\pi(\theta)$ or $p(\theta)$ as the prior distribution.
\end{note}

\begin{note}
    For any $\theta \in \Theta, \; y \in Y$, $\bf{P}(y|\theta)$ describes the sampling model 
\end{note}

\begin{note}
    Let $\theta \in \Theta$, the posterior distribution $\bf{P}(\theta | y)$ describes our belief about the parameters based on samples.
\end{note}

\begin{theorem}[Bayes' Rule]
    \begin{equation}
        \bf{P}(\theta|y) = \frac{\bf{P}(y|\theta)\pi(\theta)}{\bf{P}(y)}
    \end{equation}
\end{theorem}

\begin{example}
    Suppose $\theta \in [0,1]$, and $Y_i|\theta \sim Bernoulli(\theta)$ with sample size 20. Then let $y|\theta = \sum Y_i \sim Binomial(20, \theta)$. We will see how the choice of the prior has on the posterior with $\theta \sim Beta(a,b)$, we have
    
    \begin{equation*}
        \mathbb{E}[\theta] = \frac{a}{a+b}, \qquad mode[\theta] = \frac{a-1}{a-1+b-1}
    \end{equation*}

    Usually, from a prior sampling, we denote $a$ be the number of events counted, and $b$ as the total sample size. In this case, say the event happens twice, we have

    \begin{equation*}
        \theta \sim Beta(2, 20)
    \end{equation*}

    We know that the pdf of $Beta(a,b)$ is 

    \begin{equation*}
        pdf(\theta) = \frac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)}\theta^{a-1}(1-\theta)^{b-1}, \; \theta \in [0,1]
    \end{equation*}

    Using Bayes' Rule, we have 
    \begin{align*}
        \bf{P}(\theta|Y_1, \ldots, Y_{20}) &\propto \bf{P}(Y_1, \ldots, Y_{20}|\theta) \cdot \pi(\theta) \\
        &\propto \theta^{\Sigma Y_i}(1-\theta)^{n - \Sigma Y_i} \cdot \theta^{a-1}(1-\theta)^{b-1} \\
        &=\theta^{a + y - 1} (1-\theta)^{b + n - y - 1}\\
        &\sim Beta(a + y, b + n - y)
    \end{align*}

    This means, amazingly, the posterior falls in the same family of distribution like the prior. For priors like this, we give them a special name: Conjugate Prior. 
\end{example}

From the example, we can derive
\begin{equation*}
    \mathbb{E}[\theta] = \frac{a}{a+b}, \qquad mean(sample) = \frac{y}{n}, \qquad \mathbb{E}[\theta | y] = \frac{a + y}{a+ b + n}
\end{equation*}

With a little massage, we have
\begin{equation*}
    \mathbb{E}[\theta|y] = \frac{a+b}{a+b+n} \cdot \frac{a}{a+b} + \frac{n}{a+b+n} \cdot \frac{\sum y_i}{n}
\end{equation*}

This break down is intriguing because the posterior mean is in fact the weighted sum of the prior mean and sample mean, implying insensitivity to the prior as $n\rightarrow \infty$ since the weight dominates. We can further conclude the above example with the following proposition:

\begin{proposition}\label{Prop:BernoulliBeta_Conjugate_Prior}
    With $Y_i|\theta \sim Bernoulli(\theta)$ and $\theta \sim Beta(a,b)$ as the conjugate prior, we have the posterior $\theta | Y_1, \ldots, Y_n \sim Beta(a + \sum Y_i, b + n - \sum Y_i)$
\end{proposition}

\begin{remark}
    \begin{equation*}
        Uniform[0,1] = Beta(1,1)
    \end{equation*}
\end{remark}

\section{One-parameter Models}

\subsection{Sufficient Statistics and Conjugate Prior}

In this section, we talk about single-parameter models, where the example in \autoref{Prop:Bernoulli+Beta Conjugate Prior} about Bernoulli with Beta priors was a perfect example. Let's start with a closer look at the posterior with uniform prior:
\begin{equation*}
    \bf{P}(\theta | Y_1, \ldots, Y_n) 
    \;\propto\; \bf{P}(Y_1, \ldots, Y_n | \theta) \cdot \pi(\theta) 
    \; = \; \theta^{\Sigma Y_i}(1-\theta)^{n - \Sigma Y_i}
\end{equation*}
By observing the last term, the posterior distribution is determined by the statistic $\sum Y_i$(we assume sample size known at all time). This means we don't need to examine the exact values of $Y_i$, but the sum would be enough/sufficient to find out the parameters of the posterior. As a result, we say the sum $\sum Y_i$ is the sufficient statistic of the posterior distribution.

\begin{definition}[Sufficient Statistics]
    Given any subject $\mathcal{S}$ we are trying to estimate, a distribution, a parameter, or even another statistic, a statistic $T(Y_i)$ is a sufficient statistic of $\mathcal{S}$ if $T(Y_i)$ contains enough information for us to determine that subject.
\end{definition}

In the previous section, we also mentioned the rough idea of a conjugate prior, now we give it a formal definition:
\begin{definition}[Conjugate Prior]
    A class of prior distributions $\mathcal{P}$ for $\theta$ is called conjugate for a sampling model $\bf{P}(Y|\theta)$ if
    \begin{equation*}
        \pi(\theta) \in \mathcal{P} \Rightarrow \bf{P}(\theta | Y) \in \mathcal{P}
    \end{equation*}
\end{definition}

Now we see how these two concepts play together with the following example.
\begin{example}
    Previously we have talked about the posterior conditioned over the entire sequence, $\theta | Y_1, \ldots, Y_n$. What would happen if we instead condition the parameter with posterior's sufficient statistic? i.e. $\theta | y=\sum_{i=1}^{n}Y_i$

    \begin{align*}
        \bf{P}(\theta | y) &\propto \bf{P}(y | \theta) \cdot \pi(\theta) \\
        &= \binom{n}{y} \theta^{y}(1-\theta)^{n-y} \theta^{a-1}(1-\theta)^{b-1} \\
        &\propto \theta^{a + y - 1}(1-\theta)^{b + n - y - 1} \\
        &\sim Beta(a+y, b+n-y)
    \end{align*}

    Surprisingly, it looks the same as $\theta | Y_1, \ldots, Y_n$! This is because $y$, as the sufficient statistic for the posterior, is enough to determine the distribution.
\end{example}

\subsection{Predictive Distribution}
There are two main reasons why we model things with mathematics in general: to explain, and to predict. In this section, we explore how can we make predictions within the Bayesian framework.

\begin{definition}[Predictive Distribution]
    Given data points $Y_1, \ldots, Y_n$, the predictive distribution refers to
    \begin{equation*}
        Y_{n+1}|Y_1, \ldots, Y_n
    \end{equation*}
\end{definition}

\begin{example}
    Say we have $Y_i | \theta \sim Bernoulli(\theta)$ and prior $\pi(\theta) \sim Beta(a,b)$, then the predictive distribution $Y_{n+1} | Y_1, \ldots, Y_n \sim Bernoulli(\hat{\theta})$, and we will try to find the exact value of $\hat{\theta}$ in order to determine the predictive distribution. Using marginalization, we have
    \begin{align*}
        \hat{\theta} &= \bf{P}(Y_{n+1} = 1 | Y_1, \ldots, Y_n) \\
        &= \int_0^1 \bf{P}(Y_{n+1}=1, \theta | Y_1, \ldots, Y_n) d \mathbb{P}(\theta | Y_1, \ldots, Y_n) \\
        &= \int_{0}^{1} \theta \cdot pdf(Beta(a + \sum Y_i, b + n - \sum Y_i)) d\theta \\
        &= \mathbb{E}[\theta | Y_1, \ldots, Y_n] \\
        &= \frac{a + \sum Y_i}{a + b + n}
    \end{align*}
    As a result, we have our predictive distribution $Y_{n+1} | Y_1, \ldots, Y_n \sim Bernoulli(\frac{a + \Sigma Y_i}{a + b + n})$
\end{example}


\subsection{Confidence Regions}
After we have known how to estimate the next observation with predictive distribution, now let's find out how to estimate the parameters. 

If we were frenquencists, we first assume the sample distribution, with which then determine the confidence interval(also random variables), after that we sample and see if the result lies in the interval, which determines whether we reject hypothesis $H_0$.

However, as a Bayesian, the confidence of our estimation on $\theta$ originates from the posterior $\bf{P}(\theta | Y_1, \ldots, Y_n)$, which means we have to sample before determining the "interval" or in essence, the distribution itself. Also, we are able to tell the odds very clearly because right now we know exactly how $\theta | Y_1, \ldots, Y_n$ distributes, at least that's what we hope for. 

Moreover, in the Bayesian way, there are two major ways to determine the interval with the posterior distribution. 
\begin{enumerate}
    \item Highest Posterior Density(HPD): points with the highest of the posterior pdf, also $\mathbb{P}(HDP_\alpha) = 1 - \alpha$.
    \item Quantile based interval: old-fashion $(\theta_{\alpha/2}, \theta_{1-\alpha/2})$
\end{enumerate}

\subsection{Case Study: Possion + Gamma Prior}
Now we are in business, in this section, we introduce another pair of conjugate prior. A little recap, if $Y \sim Poission(\theta)$, then $\bf{P}(Y=y | \theta) = \frac{e^{-\theta}\theta^y}{y!}, \; y \in \b{N}$. We will leave the pdf of Gamma for now because in this case study, we present a way to actually find the distribution. Firstly, a little analysis on the joint sampling distribution:

\begin{align*}
    \bf{P}(Y_1, \ldots, Y_n | \theta)  &= \prod_{i=1}^{n}\bf{P}(Y_i | \theta) \\
    &= \prod_{i=1}^{n}\frac{e^{-\theta}\theta^{Y_i}}{Y_i!} \\
    &= \frac{e^{-n\theta}\theta^{\Sigma Y_i}}{\prod_{i=1}^{n} Y_i!}
\end{align*}

\begin{note}
    $Y = Y_1 + \cdots + Y_n \sim Possion(n\theta)$.
\end{note}

Right now we can work on the posterior.

\begin{align}
    \bf{P}(\theta | Y_1, \ldots, Y_n) &\propto \pi(\theta) \cdot \bf{P}(Y_1, \ldots, Y_n | \theta) \\
    &\propto \pi(\theta) \cdot e^{c_1\theta}\theta^{c_2} \label{eq:PossionGamma_Prior}
\end{align}

Let's guess what the conjugate prior should look like. If $\pi(\theta) \propto e^{d_1\theta}\theta^{d_2}$, then the prior and the posterior will be proportionate to the same pattern, i.e. $\bf{P}(\theta | Y_1, \ldots, Y_n) \propto e^{(c_1 + d_1)\theta}\theta^{(c_2 + d_2)}$. Then all we need to do now is to determine the constant depending on $c_i, d_i$, the result is
\begin{equation*}
    \pi(\theta) \sim Gamma(a,b) = \frac{b^a}{\Gamma(a)}\theta^{a-1}e^{-b\theta}
\end{equation*}
Therefore, we can push further with \ref{eq:PossionGamma_Prior} so we have
\begin{equation*}
    \bf{P}(\theta | Y_1, \ldots, Y_n) \propto Gamma(a + \Sigma Y_i, b+n)
\end{equation*}
And we conclude this case study with the following proposition

\begin{proposition}\label{prop:PoissionGamma_Conjugate_Prior}
    With $Y_i|\theta \sim Poission(\theta)$ and $\theta \sim Gamma(a,b)$ as the conjugate prior, we have the posterior $\theta | Y_1, \ldots, Y_n \sim Gamma(a + \Sigma Y_i, b + n)$.
\end{proposition}


\section{Exponential Family and Monte Carlo Method}
In previous examples, we have seen two conjugate distribution pairs that saves our lives from tedious computation. A natural question one may ask is, does there exist a pattern for us to find conjugate priors given any sampling distribution? The answer is almost yes, there's indeed a pattern for huge family of distributions: the exponential family.

\begin{definition}[One-parameter Exponential Family]
    A sampling model $\bf{P}(Y|\theta)$ is an one-parameter exponential family model if we have the following decomposition:
    \begin{equation}\label{def:ExponentialFamily}
        \bf{P}(Y|\theta) = h(Y)c(\theta)\exp(\theta t(Y))
    \end{equation}
    where $\theta$ is a parameter, and $t(Y)$ is the sufficient statistic for the posterior.
\end{definition}

In imitation to (\ref{def:ExponentialFamily}), with $n_0, t_0$ being the sample size and sufficient statistic of the prior sample, we suppose our prior takes the form:
\begin{equation*}
    \pi(\theta) = k(n_0, t_0)c(\theta)^{n_0}\exp(\textcolor{red}{n_0}\textcolor{blue}{t_0}\textcolor{purple}{\theta})
\end{equation*}

We then can derive the posterior distribution using the Bayes' Rule:
\begin{align*}
    \bf{P}(\theta | Y_1, \ldots, Y_n) &\propto \pi(\theta) \cdot \bf{P}(Y_1, \ldots, Y_n | \theta) \\
    &\propto c(\theta)^{n_0+n}\exp\{ \textcolor{red}{(n_0 + n)} \textcolor{blue}{\frac{n_0t_0 + n\bar{t(y)}}{n_0 + n}}\textcolor{purple}{\theta} \}
\end{align*}

With the color, we can easily see that the prior and the posterior fall into the same family of distribution, which by definition is conjugate.

As for Monte Carlo, it's just a fancy name for simulation/experiment. Specifically, it applies to the field of Bayesian by allowing us to simulate the sampling process. Given a joint distribution $\bf{P}(Y, \theta)$, we can rewrite it as $\pi(\theta)\bf{P}(Y|\theta)$. There are 3 steps for us to follow, given sample $Y_i$:
\begin{enumerate}
    \item Sample a list of parameter $\vec\theta = \{\theta_1, \ldots, \theta_n\}$ with $\theta_i \sim \pi(\theta | Y_1, \ldots, Y_{n_0})$
    \item For each $\theta_i$, sample $\tilde{Y_i} \sim \bf{P}(Y | \theta, Y_1, \ldots, Y_{n_0})$
    \item return $\{(\theta_i, \tilde{Y_i})\}_{i=1}^n$
\end{enumerate}

Sometimes we may find our simulation does not align with our empirical data. Firstly, it's possible we were unlucky, so we can run this entire experiment $k$ times, meaning a total of $k \cdot n$ pairs of $(\theta, Y)$ will be generated. It's also possible that we need to change our model, both the sampling model and the prior have room for adjustment.