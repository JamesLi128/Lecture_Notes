\chapter{Proximal Method}
In this chapter we will study proximal method which can be seen as a generalization of gradient descent. Before formally introduce the formulation, we will first describe a few examples and how the idea is developed. 

\section{Motivating Problems}
Many problems in real life can be ill-posed due to the lack of information. For example, in solving the linear system

\begin{equation*}
    Ax = b, \quad A \in \mathbb{R}^{n \times p}
\end{equation*}
It can be solved if $A$ is tall and thin, i.e. $n \gg p$. But for most of the time in science, what available is a fat and short one. This means the matrix $A$ is probably not full-rank, which means the solution is not unique. But picking from a space of solution can be subjective sometimes and hard to justify. A natural idea is to add additional restrictions(structure) on top of the existing problem so as to narrow down the parameter space where we are exploring for answers. In this way, we can hope/guarantee the uniqueness of the solution. One way to solve this problem is through adding $L^1$ norm restriction on the parameters $x$, which is also famously known as Lasso regression:
\begin{equation*}
    \min_{x \in \mathbb{R}^d} \|Ax - b\|^2 + \lambda\|x\|_1
\end{equation*}
By adding this restriction, we are favoring smaller parameters in hope for better robustness. 

Another interesting exmaple is the Netflix Recommendation System challenge which can be mathematically formulated as a matrix recovery problem. Given a matrix $X \in \mathbb{R}^{d_1 \times d_2}$ such that
\begin{equation*}
    \mathcal{A}(X) = b
\end{equation*}
where operator $\mathcal{A}$ maps $X \mapsto \mathbb{R}^m$ with $m \ll d_1 \times d_2$. Similarly, in order to preserve the low-rank property, the problem is formulated as:
\begin{equation*}
    \min \|\mathcal{A}(X) - b \|^2_2 + \lambda\|X\|_*
\end{equation*}
where $\| X \|_* = \sum_{i=1}^{\min d_1, d_2}\sigma_i(X)$ refers to the nuclear norm.

\begin{remark}
    The common pattern between the above two examples is that they are all formed in the following way
    \begin{equation*}
        \min_{x} f(x) + h(x)
    \end{equation*}
    where $f(x)$ is smooth/differentiable, and $h(x)$ is convex and has a nice decomposition. 
\end{remark}

This insight leads us into the object we are going to study.

\section{Proximal Operator}
Algorithms are usually inspired by the idea of approximation, just like gradient descent. We proved that the update rule
\begin{equation*}
    x_{k+1} \leftarrow x_k - \alpha_k \nabla f(x_k)
\end{equation*}
is equivalent to solving the following approximation 
\begin{equation*}
    x_{k+1} \leftarrow \argmin \{ f(x_{k}) + \langle \nabla f(x_k), x-x_k \rangle + \frac{1}{2\alpha_k}\|x - x_k\|^2 \}
\end{equation*}

As a result, gradient descent can be viewed as a special case of proximal method where $f(x) = f(x_{k}) + \langle \nabla f(x_k), x-x_k \rangle$ is differentiable, and $h(x) = \frac{1}{2\alpha_k}\|x - x_k\|^2$ is convex. To formulate this, we consider any given closed convex function $h : \mathbb{R}^d \rightarrow \mathbb{R}\cup\{+\infty\}$, the proximal operator is defined as
\begin{equation}
    prox_{\alpha h}(x) = \argmin_{z \in \mathbb{R}^d} \{ h(z) + \frac{1}{2\alpha}\|z - x \|^2 \}
\end{equation}

One thing that needs clarification is that we used $=$ instead of $\in$ in the definition of the proximal operator, this is guaranteed by the fact that $h(z) + \frac{1}{2\alpha}\|z - x \|^2$ is a strongly convex function, resulting in a unique minimizer. This gives the following statement.
\begin{lemma}
    The proximal operator is well defined.
\end{lemma}

The proximal operator can be also seen as a special case of the following configuration:
\begin{equation*}
    \argmin_{x \in \mathbb{R}^d} f(x) + h(x)
\end{equation*}
where $f(x)$ is smooth and $h(x)$ closed convex. We have the following necessary condition for this optimization problem. 
\begin{lemma}\label{lemma:CompositeMinNecessary}
    Let $h : \mathbb{R}^d \rightarrow \mathbb{R}\cup\{+\infty\}$ be a closed convex function and $f : \mathbb{R}^d \rightarrow \mathbb{R}$ be a smooth function. Let $x^* \in \argmin f(x) + h(x)$, then we have
    \begin{equation*}
        - \nabla f(x^*) \in \partial h(x^*)
    \end{equation*}
\end{lemma}
\begin{proof}
    The proof is done by using the definition of subdifferential, the definition of the gradients, and that $x^*$ is the minimizer. 
\end{proof}
\begin{note}
    We can't prove this using subdifferential calculus since it requires both $f(x)$ and $h(x)$ to be convex. 
\end{note}

Furthermore, if we ask $f(x)$ to be also convex, then this is not only necessary, but the sufficient condition. 
\begin{lemma}
    Let $h(x)$ be closed convex and $f(x)$ smooth convex. Then we have
    \begin{equation*}
        x^* \in \argmin_x f(x) + h(x) \iff -\nabla f(x^*) \in \partial h(x^*)
    \end{equation*}
\end{lemma}

As a special case of the above lemma, where $f(z) = \frac{1}{2\alpha}\|x-z\|^2$ is smooth convex, the output of proximal operator satisfies the necessary sufficient condition.
\begin{corollary}\label{corollary:ProximalOperatorNecessarySufficient}
    \begin{equation*}
        x^+ = prox_{\alpha h}(x) \iff \frac{x - x^+}{\alpha} \in \partial h(x^+)
    \end{equation*}
\end{corollary}


\section{Interpretation on Proximal Operator}
The interpretation can be explain from two different perspectives, the first one arises from the analysis of the general composite optimization problem
\begin{equation*}
    \argmin_{x \in \mathbb{R}^d} f(x) + h(x)
\end{equation*}
with smooth $f(x)$ and closed convex $h(x)$. 
\subsection*{First View}
Naturally we want to update our step by gradient descent, but the non-differentiablility of $h(x)$ stops us from doing so. As a result, we take a step back and try to analyze the approximation problem by substituting $f(x)$ with $f_k(x)$, where
\begin{equation*}
    f_k(x) = f(x_k) + \langle \nabla f(x_k), x-x_k \rangle + \frac{1}{2\alpha_k} \| x - x_k \|^2
\end{equation*}
We introduce iteration update with footnote $k$ because we are not making exact minimization calculation but an approximation, and hoping to obtain some convergence behavior with big $k$. Right now the problem of exact minimization is reduced to the following iteration update
\begin{equation*}
    x_{k+1} \leftarrow \argmin_{x \in \mathbb{R}^d} f_k(x) + h(x)
\end{equation*}
By using the necessary condition lemma \ref{lemma:CompositeMinNecessary}, we derive the following relation
\begin{equation*}
    \frac{x_k - \alpha_k \nabla f(x_k) - x_{k+1}}{\alpha_k} \in \partial h(x_{k+1})
\end{equation*}
This is quite indicative, because by referring to corollary \ref{corollary:ProximalOperatorNecessarySufficient}, we actually get
\begin{equation}
    x_{k+1} = prox_{\alpha_k h}(x_k - \alpha_k \nabla f(x_k)) \label{eq:ProximalUpdateRule}
\end{equation}

\subsection*{Second View}
The second interpretation stems from equation \ref{eq:ProximalUpdateRule}. It is easy to see that the input of the proximal operator is in fact one step of gradient descent with respect to $f(x)$ only. Then based on this gradient descent updated point $x_k - \alpha_k \nabla f(x_k)$, we try to update $h(x)$ with an additional penalty who prevents the second update from going too crazy. The second step is achieved through the proximal operator. 


\section{Forward-Backward Method}
Now let's consider the general optimization problem
\begin{equation*}
    \min_{x \in \mathbb{R}^d} f(x) + h(x)
\end{equation*}
with smooth $f(x)$ and closed convex $h(x)$. Recall that gradient descent can be derived as a proximal operator, similarly, for this problem we can derive the following linear approximation
\begin{equation*}
    x_{k+1} \leftarrow \argmin_{x \in \mathbb{R}^d} \underbrace{h(x)}_{\text{closed convex}} + \underbrace{f(x_k) + \langle \nabla f(x_k), x-x_k \rangle + \frac{1}{2\alpha_k} \| x - x_k \|^2}_{\text{smooth}}
\end{equation*}

Then by repeating what we have done in the previous section, we have the backward
\begin{equation*}
    \frac{x_k - \alpha_k\nabla f(x_k) - x_{k+1}}{\alpha_k} \in \partial h(x_{k+1})
\end{equation*}
which can be rewritten in terms of the proximal operator as the forward
\begin{equation*}
    x_{k+1} \leftarrow prox_{\alpha_kh}(x_k - \alpha_k \nabla f(x_k))
\end{equation*}
\begin{remark}
    The forward-backward method is used to find out the approximate minimizer for the big problem of minimizing $f(x) + h(x)$, but it's only useful when solving the prox is efficient.
\end{remark}

\begin{example}
    If we take $h(x) = \| x \|_1$, then we have
    \begin{align*}
        \big[ prox_{\alpha \| \cdot \|_1} \big]_i = 
        \left\{
        \begin{array}{ll}
            x_i + \alpha&, x_i < -\alpha \\
            0&, x_i \in [-\alpha, \alpha] \\
            x_i - \alpha&, x_i > \alpha \\
        \end{array}
        \right.
    \end{align*}
\end{example}

Now we examine the convergence guarantee for the forward-backward method. First, we define the gradient mapping by
\begin{equation*}
    G_\alpha(x) = \frac{1}{\alpha}(x - prox_{\alpha h}(x - \alpha \nabla f(x))) = \frac{x - x^+}{\alpha}
\end{equation*}

Observe that if $x$ is the minimizer, then with $x^+ = prox_{\alpha h}(x - \alpha \nabla f(x))$ we must have 
\begin{equation*}
    0 = G_\alpha(x) \in \partial h(x^+) + \{ \nabla f(x) \}
\end{equation*}

This implied $x = x^+$ and $-\nabla f(x) \in \partial h(x^+)$. As a result, in practice we will use $\| G_\alpha(x) \|$ as the measure of optimality. And we have the following descent lemma 2.0
\begin{lemma}[Descent Lemma for FBM]
    Assume $f(x)$ is L-smooth, $h(x)$ closed convex, then for all $x \in \mathbb{R}^d$ we have
    \begin{equation*}
        (f + h)(x^+) \leq (f + h)(x) - (\alpha - \frac{L\alpha^2}{2})\|G_\alpha(x)\|^2
    \end{equation*}
\end{lemma}

As a corollary, we can use descent lemma to bound the average norm of $G_\alpha(x)$
\begin{corollary}
    For $f(x)$ L-smooth and $h(x)$ closed convex, by setting $\alpha = \frac{1}{L}$ in FBM, we have
    \begin{equation*}
        \frac{1}{T}\sum_{i=1}^{T-1}\| G_{1/L}(x_k)\|^2 \leq \frac{2L}{T}((f+h)(x_0) - \min_{x \in \mathbb{R}^d} (f+h))
    \end{equation*}
\end{corollary}

Furthurmore, if we add convexity to $f(x)$ we have better guarantee.
\begin{theorem}
    Let $f(x)$ be both convex and L-smooth and $h(x)$ being closed such that $x^* \in \argmin (f + h)(x)$, then the iterates of FBM with $\alpha_k = \frac{1}{L}$ satisfy
    \begin{equation*}
        (f + h)(x_{k+1}) - \min (f + h) \leq \frac{L}{2k}\|x_0 - x^*\|^2
    \end{equation*}
\end{theorem}

On top of that, if we enforce strong convexity on $f(x) + h(x)$, we get upper bound in terms of 
\begin{theorem}
    If $f+h$ is $\mu$-strongly convex, then we have 
    \begin{equation*}
        (f+h)(x_{k+1}) - \min f+h \leq \frac{L}{\mu k} [(f+h)(x_0) - \min (f+h)]
    \end{equation*}
\end{theorem}

Finally, we have the accelerated FBM and its convergence guarantee.

\begin{minipage}{0.8\textwidth}  % Set width to 80% of text width
    \centering
    \begin{algorithm}[H]
        \caption{Accelerated Forward Backward Method}
        \KwIn{$x_0$, $f(x)$, $h(x)$, $K$, $L$} % Input
        \KwOut{Minimizer $x_K$}  % Output
    
        \textbf{Initialize:} $\lambda_0 \gets 0$, $y_0 \gets x_0$\;
    
        \For{$k = 0$ \KwTo $K-1$}{
            $y_{k+1} \gets prox_{\alpha h}(x_k - \frac{1}{L}\nabla f(x_k))$\;
            $\lambda_{k+1} \gets \frac{1 + \sqrt{1 + 4\lambda_k^2}}{2}$\;
            $\gamma_k \gets \frac{\lambda_k - 1}{\lambda_{k+1}}$\;
            $x_{k+1} \gets y_{k+1} + \gamma_k (y_{k+1} - y_k)$\;
        }
        \Return{$x_K$}
    \end{algorithm}
    \vspace{2em}
\end{minipage}

\begin{theorem}
    Given any convex and L-smooth $f(x)$ and closed convex $h(x)$, by setting $\alpha_k = 1/L$, the iterates of AFBM satisfy
    \begin{equation*}
        (f+h)(y_k) - \min(f+h) \leq \frac{2L \|x_0 - x^*\|^2}{(k+1)^2}
    \end{equation*}
\end{theorem}

\section{Constrained Optimization and Alternating Projection}
Sometimes we find it practical to restrain the search space to be within a set $S$ instead of the entire $\mathbb{R}^d$. In this case, the optimization problem is formulated as
\begin{equation*}
    \min_{x \in S} f(x)
\end{equation*}

However, with a little help from the idicator function, we can easily transform that into an unconstrained optimization problem
\begin{equation*}
    \min_{x \in \mathbb{R}^d} f(x) + \iota_S(x)
\end{equation*}

Moreover, if set $S$ is convex, which implies the convexity of $\iota_S(x)$, then this problem can be reduced to what we just talked about in the previous section. One interesting observation is that, for any set $S$ we have the following interpretation.
\begin{lemma}
    $$prox_{\alpha \iota_S}(x) = Proj_S(x)$$
\end{lemma}

This means, in this case, the forward-backward method is equivalent to projected gradient descent, which means
\begin{equation*}
    x_{k+1} \leftarrow Proj_S(x_k - \alpha_k \nabla f(x_k))
\end{equation*}

From this point, we look at a new constrained problem, with convex sets $C_1$ and $C_2$ 
\begin{equation*}
    \min_{x \in C_1, y \in C_2} \| x - y \|
\end{equation*}

Then with $P_{C_i}$ as the projection to set $C_i$, John Von Novmann proposes the following update rule, with an arbitrary starting point $x_0$, we project between/alternate between $C_1$ and $C_2$.
\begin{equation*}
    x_{k+1} \leftarrow P_{C_1}P_{C_2}(x_k)
\end{equation*}

With the number of iteration sufficiently large, we have one of the following two happens
\begin{enumerate}
    \item It converges to some point in $C_1 \cap C_2$
    \item $\|x - y\|$ converges to some positive number
\end{enumerate}
In both cases, with $F$ being the alternating projection, what we are trying to find is a fixed point $x^*$ where 
\begin{equation*}
    x^* = F(x^*)
\end{equation*}

Moreover, we can derive the following average convergence without any additional information
\begin{theorem}
    The iterates of alternating projection satisfy
    \begin{equation*}
        \frac{1}{T}\sum_{k=0}^{T-1}\|x_k - F(x_k)\|^2 \leq \frac{\|x_0 - x^*\|^2}{T}
    \end{equation*}
\end{theorem}

\section{Relation between Newton's Method and Projection}
Let sets $C_1 = \{ (x, y) : y \leq 0\}$ and $C_2 = epi(f)$ with $f(x)$ closed convex. For the sake of arguement, let's assume there exists some $x^* \in \mathbb{R}$ such that $f(x^*) = 0$ and $f(x) > 0, \; \forall x > x^*$. In other words, we will analyze the Newton's method from the right hand side of $x^*$.

As shown in the figure, we start at some $p_n$ on the boundary of $C_2$, where $p_n = (x_n, f(x_n)) \in epi(f)$. According to the update rule of Newton's method, we have
\begin{equation*}
    x_{n+1} \leftarrow x_n - \frac{f(x_n)}{s_n}
\end{equation*}
where $s_n \in \partial f(x_n)$. We will prove that, by firstly projecting $p_n$ from $C_2$ to $C_1$ we obtain $(x_n, 0) = Proj_{C_1}(p_n)$, and then projecting $(x_n, 0)$ back to $C_2$ where $p_{n+1} = Proj_{C_2}((x_n, 0)) = (\hat{x}_{n+1}, f(\hat{x}_{n+1}))$, we have 
\begin{equation*}
    x_{n+1} = \hat{x}_{n+1}
\end{equation*}

To begin with, it's quite easy to see why the projection of $p_n$ to $C_1$ is $(x_n, 0)$. Then, a key observation is that the direction of $(x_n, 0) - p_{n+1}$ points to the same direction as the normal vector of the supporting hyperplane at $p_{n+1}$. 

By denoting $(a, r)$ as the normal vector associated with the supporting hyperplane at $p_{n+1}$, we know that there exists some $\lambda > 0$ such that $p_{n+1} + \lambda (a, r) = (x_n, 0)$. This results in the following system
\begin{align*}
    \left\{
    \begin{array}{ll}
        x_{n+1} + \lambda a &= x_n \\
        f(x_{n+1}) + \lambda r &= 0 \\
    \end{array}
    \right.
\end{align*}
This gives us
\begin{equation*}
    x_{n+1} = x_n + \frac{f(x_)}{}
\end{equation*}