\chapter{Proximal Method}
In this chapter we will study proximal method which can be seen as a generalization of gradient descent. Before formally introduce the formulation, we will first describe a few examples and how the idea is developed. 

\section{Motivating Problems}
Many problems in real life can be ill-posed due to the lack of information. For example, in solving the linear system

\begin{equation*}
    Ax = b, \quad A \in \mathbb{R}^{n \times p}
\end{equation*}
It can be solved if $A$ is tall and thin, i.e. $n \gg p$. But for most of the time in science, what available is a fat and short one. This means the matrix $A$ is probably not full-rank, which means the solution is not unique. But picking from a space of solution can be subjective sometimes and hard to justify. A natural idea is to add additional restrictions(structure) on top of the existing problem so as to narrow down the parameter space where we are exploring for answers. In this way, we can hope/guarantee the uniqueness of the solution. One way to solve this problem is through adding $L^1$ norm restriction on the parameters $x$, which is also famously known as Lasso regression:
\begin{equation*}
    \min_{x \in \mathbb{R}^d} \|Ax - b\|^2 + \lambda\|x\|_1
\end{equation*}
By adding this restriction, we are favoring smaller parameters in hope for better robustness. 

Another interesting exmaple is the Netflix Recommendation System challenge which can be mathematically formulated as a matrix recovery problem. Given a matrix $X \in \mathbb{R}^{d_1 \times d_2}$ such that
\begin{equation*}
    \mathcal{A}(X) = b
\end{equation*}
where operator $\mathcal{A}$ maps $X \mapsto \mathbb{R}^m$ with $m \ll d_1 \times d_2$. Similarly, in order to preserve the low-rank property, the problem is formulated as:
\begin{equation*}
    \min \|\mathcal{A}(X) - b \|^2_2 + \lambda\|X\|_*
\end{equation*}
where $\| X \|_* = \sum_{i=1}^{\min d_1, d_2}\sigma_i(X)$ refers to the nuclear norm.

\begin{remark}
    The common pattern between the above two examples is that they are all formed in the following way
    \begin{equation*}
        \min_{x} f(x) + h(x)
    \end{equation*}
    where $f(x)$ is smooth/differentiable, and $h(x)$ is convex and has a nice decomposition. 
\end{remark}

This insight leads us into the object we are going to study.

\section{Proximal Operator}
Algorithms are usually inspired by the idea of approximation, just like gradient descent. We proved that the update rule
\begin{equation*}
    x_{k+1} \leftarrow x_k - \alpha_k \nabla f(x_k)
\end{equation*}
is equivalent to solving the following approximation 
\begin{equation*}
    x_{k+1} \leftarrow \argmin \{ f(x_{k}) + \langle \nabla f(x_k), x-x_k \rangle + \frac{1}{2\alpha_k}\|x - x_k\|^2 \}
\end{equation*}

As a result, gradient descent can be viewed as a special case of proximal method where $f(x) = f(x_{k}) + \langle \nabla f(x_k), x-x_k \rangle$ is differentiable, and $h(x) = \frac{1}{2\alpha_k}\|x - x_k\|^2$ is convex. To formulate this, we consider any given closed convex function $h : \mathbb{R}^d \rightarrow \mathbb{R}\cup\{+\infty\}$, the proximal operator is defined as
\begin{equation}
    prox_{\alpha h}(x) = \argmin_{z \in \mathbb{R}^d} \{ h(z) + \frac{1}{2\alpha}\|z - x \|^2 \}
\end{equation}

One thing that needs clarification is that we used $=$ instead of $\in$ in the definition of the proximal operator, this is guaranteed by the fact that $h(z) + \frac{1}{2\alpha}\|z - x \|^2$ is a strongly convex function, resulting in a unique minimizer. This gives the following statement.
\begin{lemma}
    The proximal operator is well defined.
\end{lemma}

The proximal operator can be also seen as a special case of the following configuration:
\begin{equation*}
    \argmin_{x \in \mathbb{R}^d} f(x) + h(x)
\end{equation*}
where $f(x)$ is smooth and $h(x)$ closed convex. We have the following necessary condition for this optimization problem. 
\begin{lemma}\label{lemma:CompositeMinNecessary}
    Let $h : \mathbb{R}^d \rightarrow \mathbb{R}\cup\{+\infty\}$ be a closed convex function and $f : \mathbb{R}^d \rightarrow \mathbb{R}$ be a smooth function. Let $x^* \in \argmin f(x) + h(x)$, then we have
    \begin{equation*}
        - \nabla f(x^*) \in \partial h(x^*)
    \end{equation*}
\end{lemma}
\begin{proof}
    The proof is done by using the definition of subdifferential, the definition of the gradients, and that $x^*$ is the minimizer. 
\end{proof}
\begin{note}
    We can't prove this using subdifferential calculus since it requires both $f(x)$ and $h(x)$ to be convex. 
\end{note}

Furthermore, if we ask $f(x)$ to be also convex, then this is not only necessary, but the sufficient condition. 
\begin{lemma}
    Let $h(x)$ be closed convex and $f(x)$ smooth convex. Then we have
    \begin{equation*}
        x^* \in \argmin_x f(x) + h(x) \iff -\nabla f(x^*) \in \partial h(x^*)
    \end{equation*}
\end{lemma}

As a special case of the above lemma, where $f(z) = \frac{1}{2\alpha}\|x-z\|^2$ is smooth convex, the output of proximal operator satisfies the necessary sufficient condition.
\begin{corollary}\label{corollary:ProximalOperatorNecessarySufficient}
    \begin{equation*}
        x^+ = prox_{\alpha h}(x) \iff \frac{x - x^+}{\alpha} \in \partial h(x^+)
    \end{equation*}
\end{corollary}


\section{Interpretation on Proximal Operator}
The interpretation can be explain from two different perspectives, the first one arises from the analysis of the general composite optimization problem
\begin{equation*}
    \argmin_{x \in \mathbb{R}^d} f(x) + h(x)
\end{equation*}
with smooth $f(x)$ and closed convex $h(x)$. 
\subsection*{First View}
Naturally we want to update our step by gradient descent, but the non-differentiablility of $h(x)$ stops us from doing so. As a result, we take a step back and try to analyze the approximation problem by substituting $f(x)$ with $f_k(x)$, where
\begin{equation*}
    f_k(x) = f(x_k) + \langle \nabla f(x_k), x-x_k \rangle + \frac{1}{2\alpha_k} \| x - x_k \|^2
\end{equation*}
We introduce iteration update with footnote $k$ because we are not making exact minimization calculation but an approximation, and hoping to obtain some convergence behavior with big $k$. Right now the problem of exact minimization is reduced to the following iteration update
\begin{equation*}
    x_{k+1} \leftarrow \argmin_{x \in \mathbb{R}^d} f_k(x) + h(x)
\end{equation*}
By using the necessary condition lemma \ref{lemma:CompositeMinNecessary}, we derive the following relation
\begin{equation*}
    \frac{x_k - \alpha_k \nabla f(x_k) - x_{k+1}}{\alpha_k} \in \partial h(x_{k+1})
\end{equation*}
This is quite indicative, because by referring to corollary \ref{corollary:ProximalOperatorNecessarySufficient}, we actually get
\begin{equation}
    x_{k+1} = prox_{\alpha_k h}(x_k - \alpha_k \nabla f(x_k)) \label{eq:ProximalUpdateRule}
\end{equation}

\subsection*{Second View}
The second interpretation stems from equation \ref{eq:ProximalUpdateRule}. It is easy to see that the input of the proximal operator is in fact one step of gradient descent with respect to $f(x)$ only. Then based on this gradient descent updated point $x_k - \alpha_k \nabla f(x_k)$, we try to update $h(x)$ with an additional penalty who prevents the second update from going too crazy. The second step is achieved through the proximal operator. 

\subsection*{To Be Resolved}


\section{Forward-Backward Method}
Now let's consider the general optimization problem
\begin{equation*}
    \min_{x \in \mathbb{R}^d} f(x) + h(x)
\end{equation*}
with smooth $f(x)$ and closed convex $h(x)$. Recall that gradient descent can be derived as a proximal operator, similarly, for this problem we can derive the following linear approximation
\begin{equation*}
    x_{k+1} \leftarrow \argmin_{x \in \mathbb{R}^d} \underbrace{h(x)}_{\text{closed convex}} + \underbrace{f(x_k) + \langle \nabla f(x_k), x-x_k \rangle + \frac{1}{2\alpha_k} \| x - x_k \|^2}_{\text{smooth}}
\end{equation*}